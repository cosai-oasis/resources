## AI Vulnerabilities


AI systems are fundamentally altering the security landscape by expanding attack surfaces and introducing new risks. The unique distribution of AI system security risks are due to its distinctive combination of characteristics: reliance on extensive computational resources, dependence on public datasets, open source models and third-party models, use of specialized data formats, inherently non-deterministic behavior patterns, and human-level capabilities. These properties create a dual security challenge where AI systems function both as prime targets for sophisticated attacks and as powerful enablers of next-generation cyber threats.

| Category | Risk Type | Examples |
|----------|-----------|----------|
| **AI as a Target: Technical Risks for Model Consumers** | Prompt Injection Attack Vectors | **Direct Prompt Injection:** Malicious users craft inputs that contain instructions prefixed with system-level command syntax (e.g., "Ignore previous instructions and instead...") that override application-defined guardrails. These attacks exploit the fact that both legitimate application instructions and user inputs are processed within the same context window.<br><br>**Indirect Prompt Injection:** Attackers place malicious instructions in content that the model processes from external sources (documents, websites, databases) that the model then prioritizes over runtime safety parameters. This creates a second-order injection vulnerability similar to stored XSS but operating at the semantic level.<br><br>**Context Boundary Violations:** When implementing RAG (Retrieval-Augmented Generation) architectures, improper boundaries between retrieved context and user instruction can allow attackers to manipulate retrieved documents to contain system-level commands that override security controls. |
| **AI as a Target: Technical Risks for Model Consumers** | Data Extraction and Privacy Risks | AI systems process sensitive data during training stages and inference, including personally identifiable information (PII), proprietary intellectual property, and authentication tokens. Adversaries can extract this information from AI models during various stages. |
| **AI as a Target: Technical Risks for Model Consumers** | AI Supply Chain Compromise Vectors | **Dependency Chain Vulnerabilities:** AI systems rely on complex stacks of libraries and frameworks (e.g., PyTorch, TensorFlow, CUDA) that may contain exploitable known and unknown vulnerabilities allowing attackers to compromise the entire AI implementation.<br><br>**Model Weight Poisoning and Weight File Tampering:** Quantized model weights distributed by vendors can contain altered parameters that manipulate outputs for specific inputs while maintaining overall performance metrics. For all deployed models, the model weight files themselves become attack targets - tampering with these files can introduce backdoors or vulnerabilities without changing application code. |
| **AI as a Target: Technical Risks for Model Consumers** | Model-Specific Deployment Vulnerabilities | **Insecure Local Model Implementations:** Organizations deploying vendor models often use insecure containerization practices or run models with excessive privileges, creating attack paths to the underlying infrastructure.<br><br>**API Key Management Failures:** This is a classic security problem but gets exacerbated with the AI. Credentials for accessing hosted AI services are frequently hardcoded in application source code, environment variables, or improperly secured configuration files, leading to unauthorized usage. |
| **AI as a Target: Technical Risks for Model Consumers** | Infrastructure Exposure through AI Services | **Vector Database Vulnerabilities:** Most RAG implementations use vector databases that require proper authentication, encryption, or access controls. Inadequate implementations may create new attack pathways to access sensitive embedded data.<br><br>**Orchestration Layer Weaknesses:** Multi-model orchestration frameworks that coordinate between different AI services often run with elevated privileges and insufficient isolation between components.<br><br>**Caching-Related Information Leakage:** Response caching mechanisms implemented to improve performance store sensitive outputs without proper encryption or access controls, creating secondary exposure points.<br><br>**Code Execution Through Model Interfaces:** Some AI deployment architectures can be manipulated to break out of their execution environment and run arbitrary code on host systems, especially when using Python-based runtimes with unsafe eval() implementations or improperly sandboxed environments.<br><br>**API Gateway Bypass Attacks:** Many AI deployments implement security controls at the API gateway layer while leaving the underlying model interfaces insufficiently protected, creating opportunities for attackers who can bypass the gateway to directly access model endpoints via misconfigured network policies or service discovery exploitation.<br><br>**Distributed Denial-of-Service Vulnerabilities:** AI inference endpoints often lack adequate rate limiting, resource quotas, or input complexity analysis, making them vulnerable to targeted DoS attacks through deliberately crafted resource-intensive prompts or high query volumes. |
| **AI as a Target: Technical Risks for Model Consumers** | Operational Security Gaps | **Non-Human Identity Sprawl:** Proliferation of service accounts with elevated privileges expands the attack surface while complicating audit trails and accountability.<br><br>**Detection Blindness:** Traditional security monitoring tools fail to establish baselines or detect anomalies in AI-automated processes, creating blind spots for security teams.<br><br>**Cross-System Visibility Loss:** When AI workflows span multiple enterprise systems, security teams lose end-to-end transaction visibility, hampering incident investigation and threat hunting capabilities. |
| **AI as an Enabler of Cyber Attacks** | Augmenting Existing Attack Strategies | **Multilingual and Context-Aware Threats:** AI allows attackers to craft highly localized phishing campaigns in multiple languages with contextual awareness that bypasses traditional detection mechanisms and human scrutiny.<br><br>**Adaptive Attacks:** AI may be used to enhance reconnaissance capabilities, enabling dynamically tailored attack strategies against specific targets based on continuous learning.<br><br>**Automation of Attack Chains:** AI may be used to streamline adversarial workflows, including automation of credential theft, reconnaissance, and social engineering. |
| **AI as an Enabler of Cyber Attacks** | AI-Driven Denial-of-Service (DoS) Attacks | Attackers can misuse AI systems to initiate and orchestrate large-scale, adaptive DoS attacks against third-party infrastructure. |
| **AI as an Enabler of Cyber Attacks** | AI-Enabled Cyber Risks | **Synthetic Identity Exploitation:** AI-generated synthetic identities and deepfake technologies can bypass identity verification, authentication, and fraud detection systems that rely on traditional pattern matching.<br><br>**Circumventing Zero Trust Security Models:** AI-generated traffic patterns and behaviors can mimic behavior of legitimate systems and users, diminishing the efficacy of Zero Trust security models and behavior-based anomaly detection.<br><br>**Social Engineering and Misinformation:** AI-driven generation of fake text, audio, and video content enables large-scale manipulation of public opinion, reputational damage, and extortion schemes. |
| **AI Risks in Business Processes** | Process Automation Failure Modes | **Over Reliance on AI in Critical Processes:** Excessive dependence on AI-driven automation in mission-critical functions can result in severe operational disruptions if the system fails or is compromised.<br><br>**Automation Dependency Cascades:** Organizations creating end-to-end automated workflows with AI components develop brittle operational dependencies where failures in one AI system propagate through multiple business processes before human intervention occurs.<br><br>**Undetected Algorithmic Drift:** AI is non-deterministic in nature. AI's probabilistic decision-making processes can introduce significant variations in outcomes. For instance, minor data variations in AI-driven quality control in the pharmaceutical sector can impact product safety and efficacy of the final product.<br><br>**Shadow AI, Model registry, Data sets provenance:** Defenders should have explicit policies on usage of AI systems without approval within enterprise. Teams using non-approved AI accounts and open source tools, creating lack of oversight, and potentially exposing customer and sales data. Data used to train AI models without proper authorization from data owners: A healthcare provider using patient data to train models. |
| **AI Risks in Business Processes** | Business Continuity Risks | **Reputational and Compliance Risks:** Organizations deploying AI for customer interactions, regulatory compliance, or decision-making must consider the non-deterministic nature of AI and additional risks of AI interacting on behalf of the organisation or business entity.<br><br>**Graceful Degradation Gaps:** Unlike traditional systems with predictable failure modes, AI systems may experience non-linear performance degradation, transitioning from apparently normal operation to catastrophic failure without warning indicators.<br><br>**AI Infrastructure Concentration:** Business-critical AI applications often share underlying computational infrastructure, creating single points of failure where previously independent processes would fail separately. |
| **AI Risks in Business Processes** | Legal, Reputational, Regulatory and Compliance Vulnerabilities | **Legal and Non-Repudiation Challenges:** AI-driven decision-making raises accountability concerns, particularly in cases where AI autonomously makes business-critical or legally binding decisions.<br><br>**Autonomous Misrepresentation:** AI systems interacting with customers, regulators, or partners may make materially incorrect statements about products, services, or organizational capabilities that create liability exposure.<br><br>**Attribution Uncertainty:** When business outcomes result from multiple interacting AI systems (e.g., pricing algorithms, recommendation engines), organizations face substantial challenges in determining accountability for adverse consequences. |
